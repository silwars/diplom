{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc83daa0-d409-4a3e-b9e6-d29067ef2d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Init] Libraries imported.\n",
      "[Init] Window size: 0:20:00, Slide size: 0:10:00\n",
      "[Init] MIN_SUPPORT=100, MIN_CONF=0.6, ALPHA=3\n",
      "[Init] PRIMARY_FEATURES count: 15\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Импорт, параметры и выбираемые заголовки\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "from collections import defaultdict\n",
    "from datetime import timedelta\n",
    "import csv\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import time\n",
    "\n",
    "print(\"[Init] Libraries imported.\")\n",
    "\n",
    "# Window parameters\n",
    "window_size = timedelta(minutes=20)\n",
    "slide_size  = timedelta(minutes=10)\n",
    "print(f\"[Init] Window size: {window_size}, Slide size: {slide_size}\")\n",
    "\n",
    "# Discretization specification\n",
    "BINNING_SPEC = {\n",
    "    'Total Fwd Packets': ('uniform', 5),\n",
    "    'Total Backward Packets': ('uniform', 5),\n",
    "    'Total Length of Fwd Packets': ('uniform', 5),\n",
    "    'Total Length of Bwd Packets': ('uniform', 5),\n",
    "    'Active Mean': ('uniform', 5),\n",
    "    'Idle Mean': ('uniform', 5),\n",
    "    'Flow Duration': ('uniform', 5)\n",
    "}\n",
    "\n",
    "# Параметры алгоритма\n",
    "MIN_SUPPORT = 100\n",
    "MIN_CONF    = 0.6\n",
    "ALPHA       = 3\n",
    "print(f\"[Init] MIN_SUPPORT={MIN_SUPPORT}, MIN_CONF={MIN_CONF}, ALPHA={ALPHA}\")\n",
    "\n",
    "# Группы для MDFP\n",
    "FEATURE_GROUPS = {\n",
    "    'network': ['Destination IP','Destination Port','FIN Flag Count','SYN Flag Count',\n",
    "                'RST Flag Count','PSH Flag Count','ACK Flag Count'],\n",
    "    'temporal': ['Timestamp','Flow Duration'],\n",
    "    'traffic': ['Total Fwd Packets','Total Backward Packets',\n",
    "                'Total Length of Fwd Packets','Total Length of Bwd Packets',\n",
    "                'Active Mean','Idle Mean']\n",
    "}\n",
    "PRIMARY_FEATURES = set(f for grp in FEATURE_GROUPS.values() for f in grp)\n",
    "print(f\"[Init] PRIMARY_FEATURES count: {len(PRIMARY_FEATURES)}\")\n",
    "\n",
    "\n",
    "SELECTED_COLS = ['Total Fwd Packets','Total Backward Packets',\n",
    "                'Total Length of Fwd Packets','Total Length of Bwd Packets',\n",
    "                'Active Mean','Idle Mean', 'Flow Duration', 'Destination IP','Destination Port','FIN Flag Count','SYN Flag Count',\n",
    "                'RST Flag Count','PSH Flag Count','ACK Flag Count', 'Label'\n",
    "]\n",
    "\n",
    "# Флаги для отключения спама принами\n",
    "VERBOSE_MDFP   = False\n",
    "MDFP_PRINTED   = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cfd90a8-64b2-4be3-8c08-b9e98bb0d918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Load] Read DataFrame from C:\\Users\\Гребенников Матвей\\Desktop\\Диплом\\Диплом\\Code\\diplom-project\\diplom\\result\\ShortsDataSet\\label\\Friday-WorkingHours-Afternoon-DDos.pcap_ISCX_sampled1.csv, shape: (25000, 85)\n",
      "[Load] Converted 'Timestamp' to datetime.\n",
      "[Load] Sorted DataFrame by Timestamp.\n",
      "[Load] Reset DataFrame index.\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Загрузка и обработка датасета\n",
    "\n",
    "path = r'C:\\Users\\Гребенников Матвей\\Desktop\\Диплом\\Диплом\\Code\\diplom-project\\diplom\\result\\ShortsDataSet\\label\\Friday-WorkingHours-Afternoon-DDos.pcap_ISCX_sampled1.csv'\n",
    "df = pd.read_csv(path, low_memory=False)\n",
    "print(f\"[Load] Read DataFrame from {path}, shape: {df.shape}\")\n",
    "\n",
    "df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "print(\"[Load] Converted 'Timestamp' to datetime.\")\n",
    "df.sort_values('Timestamp', inplace=True)\n",
    "print(\"[Load] Sorted DataFrame by Timestamp.\")\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "print(\"[Load] Reset DataFrame index.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "680ab122-e37b-4fe5-af89-27789b7e1a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trans] Discretized selected columns, shape: (25000, 16)\n",
      "[Trans] Built string transactions, count: 25000\n",
      "[Trans] Unique items: 6066\n",
      "[Trans] Encoded global transactions, count: 25000\n",
      "[Trans] Prepared make_transactions_ids function.\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Generate and Encode Transactions (с метками bin0…binN вместо интервалов)\n",
    "\n",
    "def discretize_columns(df_in, spec, cols):\n",
    "    \"\"\"\n",
    "    Оставляем только Timestamp и указанные cols, \n",
    "    дискретизируем с uniform/quantile и метками bin0, bin1, … bin{N-1}\n",
    "    \"\"\"\n",
    "    df2 = df_in[['Timestamp'] + cols].copy()\n",
    "    for col, (method, bins) in spec.items():\n",
    "        if col not in df2 or col not in cols:\n",
    "            continue\n",
    "        arr = df2[col].astype(float)\n",
    "        # формируем метки bin0…bin{bins-1}\n",
    "        labels = [f\"bin{i}\" for i in range(bins)]\n",
    "        if method == 'quantile':\n",
    "            binned = pd.qcut(arr, q=bins, labels=labels, duplicates='drop')\n",
    "        else:  # uniform\n",
    "            binned = pd.cut(arr, bins=bins, labels=labels)\n",
    "        df2[col] = binned.astype(str).astype('category')\n",
    "    return df2\n",
    "\n",
    "# Build string transactions\n",
    "str_trans = []\n",
    "df_disc = discretize_columns(df, BINNING_SPEC, SELECTED_COLS)\n",
    "print(f\"[Trans] Discretized selected columns, shape: {df_disc.shape}\")\n",
    "for _, row in df_disc.iterrows():\n",
    "    tr = []\n",
    "    for col in SELECTED_COLS:\n",
    "        val = row[col]\n",
    "        # фильтруем нулевые флаги, если нужно\n",
    "        if 'flag count' in col.lower() and str(val).startswith('0'):\n",
    "            continue\n",
    "        tr.append(f\"{col}:{val}\")   # e.g. \"Active Mean:bin2\"\n",
    "    str_trans.append(tr)\n",
    "print(f\"[Trans] Built string transactions, count: {len(str_trans)}\")\n",
    "\n",
    "# Encode to integer IDs\n",
    "all_items = sorted({it for tr in str_trans for it in tr})\n",
    "item2id = {it:i for i,it in enumerate(all_items)}\n",
    "id2item = {i:it for it,i in item2id.items()}\n",
    "print(f\"[Trans] Unique items: {len(all_items)}\")\n",
    "# Global integer transactions\n",
    "global_trans = [[item2id[it] for it in tr] for tr in str_trans]\n",
    "print(f\"[Trans] Encoded global transactions, count: {len(global_trans)}\")\n",
    "\n",
    "\n",
    "# Function for windowed transactions (аналогично, с метками binX)\n",
    "def make_transactions_ids(df_window):\n",
    "    dfw = discretize_columns(df_window, BINNING_SPEC, SELECTED_COLS)\n",
    "    trans = []\n",
    "    for _, row in dfw.iterrows():\n",
    "        tr = []\n",
    "        for col in SELECTED_COLS:\n",
    "            if 'flag count' in col.lower() and str(row[col]).startswith('0'):\n",
    "                continue\n",
    "            item = f\"{col}:{row[col]}\"   # e.g. \"Active Mean:bin4\"\n",
    "            if item in item2id:\n",
    "                tr.append(item2id[item])\n",
    "        trans.append(tr)\n",
    "    return trans\n",
    "\n",
    "print(\"[Trans] Prepared make_transactions_ids function.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22fe64e4-ba69-4260-91fe-80b9757a487f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: COFI-Tree \n",
    "class COFINode:\n",
    "    __slots__ = ('item','count','parent','children','node_link')\n",
    "    def __init__(self, item, parent):\n",
    "        self.item = item\n",
    "        self.count = 1\n",
    "        self.parent = parent\n",
    "        self.children = {}\n",
    "        self.node_link = None\n",
    "def create_cofi_tree(transactions, min_sup):\n",
    "    freq = defaultdict(int)\n",
    "    for tr in transactions:\n",
    "        for it in tr:\n",
    "            freq[it] += 1\n",
    "    freq = {it:c for it,c in freq.items() if c>=min_sup}\n",
    "    if not freq:\n",
    "        return None, None\n",
    "    header = {it:[freq[it], None, None] for it in freq}\n",
    "    root = COFINode(None, None)\n",
    "    for tr in transactions:\n",
    "        flt = [it for it in tr if it in freq]\n",
    "        flt.sort(key=lambda x: freq[x], reverse=True)\n",
    "        node = root\n",
    "        for it in flt:\n",
    "            if it in node.children:\n",
    "                child = node.children[it]\n",
    "                child.count += 1\n",
    "            else:\n",
    "                child = COFINode(it, node)\n",
    "                node.children[it] = child\n",
    "                if header[it][1] is None:\n",
    "                    header[it][1] = child\n",
    "                    header[it][2] = child\n",
    "                else:\n",
    "                    header[it][2].node_link = child\n",
    "                    header[it][2] = child\n",
    "            node = child\n",
    "    return root, header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb50b5aa-7449-43ee-8ee5-d8688c15d2ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[COFI] Ready to mine prefixes (fully optimized).\n",
      "[COFI] Built global tree: 33 frequent items\n",
      "[COFI] Mining 33 base elements via ThreadPool...\n",
      "[COFI-base] Start base 0\n",
      "[COFI-base] Finished base 0, found 90 patterns\n",
      "[COFI-base] Start base 1\n",
      "[COFI-base] Finished base 1, found 7 patterns\n",
      "[COFI-base] Start base 258\n",
      "[COFI-base] Finished base 258, found 41 patterns\n",
      "[COFI-base] Start base 429\n",
      "[COFI-base] Finished base 429, found 36 patterns\n",
      "[COFI-base] Start base 431\n",
      "[COFI-base] Finished base 431, found 14 patterns\n",
      "[COFI-base] Start base 432\n",
      "[COFI-base] Finished base 432, found 14 patterns\n",
      "[COFI-base] Start base 433\n",
      "[COFI-base] Finished base 433, found 11 patterns\n",
      "[COFI-base] Start base 438\n",
      "[COFI-base] Finished base 438, found 59 patterns\n",
      "[COFI-base] Start base 439\n",
      "[COFI-base] Finished base 439, found 14 patterns\n",
      "[COFI-base] Start base 440\n",
      "[COFI-base] Finished base 440, found 30 patterns\n",
      "[COFI-base] Start base 442\n",
      "[COFI-base] Finished base 442, found 14 patterns\n",
      "[COFI-base] Start base 990\n",
      "[COFI-base] Finished base 990, found 23 patterns\n",
      "[COFI-base] Start base 2494\n",
      "[COFI-base] Finished base 2494, found 37 patterns\n",
      "[COFI-base] Start base 3246\n",
      "[COFI-base] Finished base 3246, found 35 patterns\n",
      "[COFI-base] Start base 5890\n",
      "[COFI-base] Finished base 5890, found 21 patterns\n",
      "[COFI-base] Start base 6034\n",
      "[COFI-base] Finished base 6034, found 12 patterns\n",
      "[COFI-base] Start base 6035\n",
      "[COFI-base] Finished base 6035, found 16 patterns\n",
      "[COFI-base] Start base 6036\n",
      "[COFI-base] Finished base 6036, found 65 patterns\n",
      "[COFI-base] Start base 6037\n",
      "[COFI-base] Finished base 6037, found 26 patterns\n",
      "[COFI-base] Start base 6038\n",
      "[COFI-base] Finished base 6038, found 44 patterns\n",
      "[COFI-base] Start base 6039\n",
      "[COFI-base] Finished base 6039, found 8 patterns\n",
      "[COFI-base] Start base 6040\n",
      "[COFI-base] Finished base 6040, found 23 patterns\n",
      "[COFI-base] Start base 6041\n",
      "[COFI-base] Finished base 6041, found 30 patterns\n",
      "[COFI-base] Start base 6042\n",
      "[COFI-base] Finished base 6042, found 23 patterns\n",
      "[COFI-base] Start base 6043\n",
      "[COFI-base] Finished base 6043, found 16 patterns\n",
      "[COFI-base] Start base 6044\n",
      "[COFI-base] Finished base 6044, found 8 patterns\n",
      "[COFI-base] Start base 6045\n",
      "[COFI-base] Finished base 6045, found 9 patterns\n",
      "[COFI-base] Start base 6046\n",
      "[COFI-base] Finished base 6046, found 8 patterns\n",
      "[COFI-base] Start base 6048\n",
      "[COFI-base] Finished base 6048, found 6 patterns\n",
      "[COFI-base] Start base 6049\n",
      "[COFI-base] Finished base 6049, found 4 patterns\n",
      "[COFI-base] Start base 6053\n",
      "[COFI-base] Finished base 6053, found 1 patterns\n",
      "[COFI-base] Start base 6057\n",
      "[COFI-base] Finished base 6057, found 1 patterns\n",
      "[COFI-base] Start base 6061\n",
      "[COFI-base] Finished base 6061, found 1 patterns\n",
      "[COFI] Global mining done: 747 patterns, 33 valid elements\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Оптимизированный COFI Mining \n",
    "\n",
    "from multiprocessing.dummy import Pool  \n",
    "from multiprocessing import cpu_count\n",
    "from collections import defaultdict\n",
    "\n",
    "print(\"[COFI] Ready to mine prefixes (fully optimized).\")\n",
    "\n",
    "def find_cofi_prefixes(item, header):\n",
    "    paths = {}\n",
    "    node = header[item][1]\n",
    "    while node:\n",
    "        cnt = node.count\n",
    "        prefix = []\n",
    "        p = node.parent\n",
    "        while p and p.item is not None:\n",
    "            prefix.append(p.item)\n",
    "            p = p.parent\n",
    "        if prefix:\n",
    "            paths[frozenset(prefix)] = paths.get(frozenset(prefix), 0) + cnt\n",
    "        node = node.node_link\n",
    "    return paths\n",
    "\n",
    "def iterate_node_links(item, header):\n",
    "    node = header[item][1]\n",
    "    while node:\n",
    "        yield node\n",
    "        node = node.node_link\n",
    "\n",
    "def build_conditional_cofi_tree(base, header, min_sup):\n",
    "    \n",
    "    freq = defaultdict(int)\n",
    "    conditional_paths = []\n",
    "    for node in iterate_node_links(base, header):\n",
    "        path = []\n",
    "        p = node.parent\n",
    "        while p and p.item is not None:\n",
    "            path.append(p.item)\n",
    "            p = p.parent\n",
    "        if path:\n",
    "            conditional_paths.append((path, node.count))\n",
    "            for it in path:\n",
    "                freq[it] += node.count\n",
    "\n",
    "    # 2) Filter infrequent items\n",
    "    freq = {it:c for it,c in freq.items() if c >= min_sup}\n",
    "    if not freq:\n",
    "        return None, None\n",
    "\n",
    "    # 3) Initialize new header and root\n",
    "    cond_header = {it:[freq[it], None, None] for it in freq}\n",
    "    root = COFINode(None, None)\n",
    "\n",
    "    # 4) Insert each conditional path into the new COFI-tree\n",
    "    for path, cnt in conditional_paths:\n",
    "        flt = [it for it in path if it in freq]\n",
    "        if not flt: continue\n",
    "        flt.sort(key=lambda x: freq[x], reverse=True)\n",
    "        node = root\n",
    "        for it in flt:\n",
    "            if it in node.children:\n",
    "                child = node.children[it]\n",
    "                child.count += cnt\n",
    "            else:\n",
    "                child = COFINode(it, node)\n",
    "                child.count = cnt\n",
    "                node.children[it] = child\n",
    "                # back-pointer insertion\n",
    "                if cond_header[it][1] is None:\n",
    "                    cond_header[it][1] = child\n",
    "                    cond_header[it][2] = child\n",
    "                else:\n",
    "                    cond_header[it][2].node_link = child\n",
    "                    cond_header[it][2] = child\n",
    "            node = child\n",
    "\n",
    "    return root, cond_header\n",
    "\n",
    "def rec_cofi(root, header, min_sup, prefix, patterns, max_depth, base, order):\n",
    "    if len(prefix) >= max_depth:\n",
    "        return\n",
    "    for it, (sup, _, _) in sorted(header.items(), key=lambda x: x[1][0]):\n",
    "        if it <= base or order[it] <= order[base]:\n",
    "            continue\n",
    "        newp = prefix | {it}\n",
    "        patterns[frozenset(newp)] = sup\n",
    "        # build and mine conditional tree recursively\n",
    "        cond_root, cond_hdr = build_conditional_cofi_tree(it, header, min_sup)\n",
    "        if cond_hdr:\n",
    "            rec_cofi(cond_root, cond_hdr, min_sup, newp, patterns, max_depth, it, order)\n",
    "\n",
    "def mine_cofi_base(args):\n",
    "    base, root, header, min_sup, max_depth, order = args\n",
    "    print(f\"[COFI-base] Start base {base}\")\n",
    "    patterns = {frozenset([base]): header[base][0]}\n",
    "\n",
    "    # Пропуск по минимальной поддержке\n",
    "    total_support = sum(node.count for node in iterate_node_links(base, header))\n",
    "    if total_support < min_sup:\n",
    "        print(f\"[COFI-base] Base {base}: conditional support {total_support} < {min_sup}, skipping\")\n",
    "        print(f\"[COFI-base] Finished base {base}, found {len(patterns)} patterns\")\n",
    "        return patterns\n",
    "\n",
    "    cond_root, cond_hdr = build_conditional_cofi_tree(base, header, min_sup)\n",
    "    if cond_hdr:\n",
    "        rec_cofi(cond_root, cond_hdr, min_sup, {base}, patterns, max_depth, base, order)\n",
    "\n",
    "    print(f\"[COFI-base] Finished base {base}, found {len(patterns)} patterns\")\n",
    "    return patterns\n",
    "\n",
    "# Глобальный COFI\n",
    "root_c, hdr_c = create_cofi_tree(global_trans, MIN_SUPPORT)\n",
    "if hdr_c:\n",
    "    print(f\"[COFI] Built global tree: {len(hdr_c)} frequent items\")\n",
    "    bases = sorted(hdr_c)\n",
    "    order = {b:i for i,b in enumerate(bases)}\n",
    "    args = [(b, root_c, hdr_c, MIN_SUPPORT, 4, order) for b in bases]\n",
    "    print(f\"[COFI] Mining {len(bases)} base elements via ThreadPool...\")\n",
    "    patterns_global = {}\n",
    "    with Pool(min(cpu_count(), len(bases))) as pool:\n",
    "        for part in pool.map(mine_cofi_base, args):\n",
    "            patterns_global.update(part)\n",
    "    valid_global = set().union(*patterns_global.keys())\n",
    "    print(f\"[COFI] Global mining done: {len(patterns_global)} patterns, {len(valid_global)} valid elements\")\n",
    "else:\n",
    "    print(f\"[COFI] No frequent items >= MIN_SUPPORT={MIN_SUPPORT}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50ccee08-d006-408c-9f6f-7eec99a09f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: MDFP-Tree Construction с параллельным подсчётом пар\n",
    "\n",
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "# (флаги и класс MDFPNode остаются без изменений)\n",
    "\n",
    "def _count_pairs_chunk(args):\n",
    "    chunk, freq = args\n",
    "    local = defaultdict(lambda: defaultdict(int))\n",
    "    for tr in chunk:\n",
    "        flt = [it for it in tr if it in freq]\n",
    "        # НЕ сортируем — combinations пройдёт по всем парам в любом порядке\n",
    "        for i,j in combinations(flt,2):\n",
    "            local[i][j] += 1\n",
    "            local[j][i] += 1\n",
    "    return local\n",
    "class MDFPNode:\n",
    "    __slots__ = ('item', 'group', 'parent', 'children', 'count', 'link', 'array')\n",
    "    def __init__(self, item, group, parent):\n",
    "        self.item     = item\n",
    "        self.group    = group\n",
    "        self.parent   = parent\n",
    "        self.children = {}\n",
    "        self.count    = 1\n",
    "        self.link     = None\n",
    "        self.array    = 0\n",
    "\n",
    "def create_mdfp_tree(transactions, min_sup):\n",
    "    \"\"\"\n",
    "    1) Считаем одиночные частоты\n",
    "    2) Фильтруем по min_sup и готовим header\n",
    "    3) Последовательно считаем pair_sup\n",
    "    4) Строим дерево\n",
    "    \"\"\"\n",
    "    # --- 1) одиночные частоты + header ---\n",
    "    freq = defaultdict(int)\n",
    "    for tr in transactions:\n",
    "        for it in tr:\n",
    "            freq[it] += 1\n",
    "    freq = {it:c for it,c in freq.items() if c >= min_sup}\n",
    "    if not freq:\n",
    "        return None, None, {}\n",
    "    header = {it:[freq[it], None, None, 0] for it in freq}\n",
    "\n",
    "    # --- 2) последовательный подсчёт пар ---\n",
    "    pair_sup = defaultdict(lambda: defaultdict(int))\n",
    "    for tr in transactions:\n",
    "        # фильтруем сразу по частотам\n",
    "        flt = [it for it in tr if it in freq]\n",
    "        # считаем для каждой пары\n",
    "        for i,j in combinations(flt, 2):\n",
    "            pair_sup[i][j] += 1\n",
    "            pair_sup[j][i] += 1\n",
    "\n",
    "    # --- 3) строим сам MDFP-дерево ---\n",
    "    root = MDFPNode(None, None, None)\n",
    "    for tr in transactions:\n",
    "        flt = [it for it in tr if it in freq]\n",
    "        if not flt:\n",
    "            continue\n",
    "        node = root\n",
    "        for key in flt:\n",
    "            if key in node.children:\n",
    "                child = node.children[key]\n",
    "                child.count += 1\n",
    "            else:\n",
    "                child = MDFPNode(\n",
    "                    key,\n",
    "                    next((g for g,v in FEATURE_GROUPS.items()\n",
    "                          if id2item[key].split(':')[0] in v), None),\n",
    "                    node\n",
    "                )\n",
    "                node.children[key] = child\n",
    "                # back-pointer\n",
    "                if header[key][1] is None:\n",
    "                    header[key][1] = child\n",
    "                    header[key][2] = child\n",
    "                else:\n",
    "                    header[key][2].link = child\n",
    "                    header[key][2] = child\n",
    "                header[key][3] += 1\n",
    "            node = child\n",
    "\n",
    "    return root, header, pair_sup\n",
    "\n",
    "\n",
    "# Cell 6b: MDFP Mining Functions\n",
    "\n",
    "def find_prefix_paths(base, header):\n",
    "    \"\"\"\n",
    "    Из header[base] по ссылкам link собираем все префикс-пути и суммируем их count.\n",
    "    \"\"\"\n",
    "    paths = {}\n",
    "    node = header[base][1]\n",
    "    while node:\n",
    "        cnt = node.count\n",
    "        prefix = []\n",
    "        p = node.parent\n",
    "        while p and p.item is not None:\n",
    "            prefix.append(p.item)\n",
    "            p = p.parent\n",
    "        if prefix:\n",
    "            paths[frozenset(prefix)] = paths.get(frozenset(prefix), 0) + cnt\n",
    "        node = node.link\n",
    "    return paths\n",
    "\n",
    "def mine_sparse(item, header, pair_sup, transactions, min_sup, k_max=4):\n",
    "    \"\"\"\n",
    "    Разреженная ветвь MDFP: генерируем L1, Lk по pair_sup без дерева.\n",
    "    \"\"\"\n",
    "    patterns = {}\n",
    "    L1 = [j for j in pair_sup[item] if pair_sup[item][j] >= min_sup]\n",
    "    for j in L1:\n",
    "        patterns[frozenset([item, j])] = pair_sup[item][j]\n",
    "    Lk_1 = [frozenset([item, j]) for j in L1]\n",
    "    k = 3\n",
    "    while Lk_1 and k <= k_max:\n",
    "        Ck = set()\n",
    "        for prev in Lk_1:\n",
    "            for j in L1:\n",
    "                if j not in prev:\n",
    "                    c = prev | {j}\n",
    "                    if len(c) == k:\n",
    "                        Ck.add(c)\n",
    "        Lk = []\n",
    "        for c in Ck:\n",
    "            cnt = sum(1 for tr in transactions if c.issubset(tr))\n",
    "            if cnt >= min_sup:\n",
    "                patterns[c] = cnt\n",
    "                Lk.append(c)\n",
    "        Lk_1, k = Lk, k+1\n",
    "    return patterns\n",
    "\n",
    "def mine_dense_item(item, root, header, min_sup):\n",
    "    \"\"\"\n",
    "    Плотная ветвь MDFP: строим условный набор транзакций и рекурсивно майним.\n",
    "    \"\"\"\n",
    "    patterns = {frozenset([item]): header[item][0]}\n",
    "    cond = find_prefix_paths(item, header)\n",
    "    trans = []\n",
    "    for pth, cnt in cond.items():\n",
    "        trans += [list(pth)] * cnt\n",
    "    if trans:\n",
    "        sub_root, sub_header, _ = create_mdfp_tree(trans, min_sup)\n",
    "        if sub_header:\n",
    "            def rec(nr, nh, prefix):\n",
    "                for it, (s, head, last, link_ct) in sorted(nh.items(), key=lambda x: x[1][0]):\n",
    "                    newset = prefix | {it}\n",
    "                    patterns[frozenset(newset)] = s\n",
    "                    cond2 = find_prefix_paths(it, nh)\n",
    "                    t2 = []\n",
    "                    for p2, c2 in cond2.items():\n",
    "                        t2 += [list(p2)] * c2\n",
    "                    if t2:\n",
    "                        sr, sh, _ = create_mdfp_tree(t2, min_sup)\n",
    "                        if sh:\n",
    "                            rec(sr, sh, newset)\n",
    "            rec(sub_root, sub_header, {item})\n",
    "    return patterns\n",
    "\n",
    "def mine_mdfp(transactions, min_sup):\n",
    "    \"\"\"\n",
    "    Основная точка входа для майнинга MDFP: строим дерево, \n",
    "    разделяем элементы на sparse/dense и собираем все patterns.\n",
    "    \"\"\"\n",
    "    root, header, pair_sup = create_mdfp_tree(transactions, min_sup)\n",
    "    if not header:\n",
    "        return {}\n",
    "    # разделяем на разреженные и плотные\n",
    "    sparse = [it for it, (sup, head, last, link_ct) in header.items() if head and head.array == 1]\n",
    "    dense  = [it for it in header if it not in sparse]\n",
    "    patterns = {}\n",
    "    for it in sparse:\n",
    "        patterns.update(mine_sparse(it, header, pair_sup, transactions, min_sup))\n",
    "    for it in dense:\n",
    "        patterns.update(mine_dense_item(it, root, header, min_sup))\n",
    "    return patterns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c5015a6-b51c-4828-8a51-9827cbdf2940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Pattern Filtering and Rule Generation\n",
    "\n",
    "def filter_closed(patterns):\n",
    "    closed = {}\n",
    "    for p, s in patterns.items():\n",
    "        if not any(len(q) > len(p) and p.issubset(q) and patterns[q] == s\n",
    "                   for q in patterns):\n",
    "            closed[p] = s\n",
    "    print(f\"[Filter] Closed patterns count: {len(closed)}\")\n",
    "    return closed\n",
    "\n",
    "def generate_rules(patterns, min_conf, transactions, label_id):\n",
    "    rules = []\n",
    "    total_tx = len(transactions)\n",
    "    # сколько раз в окне вообще встречается метка атаки\n",
    "    sup_label = sum(1 for tr in transactions if label_id in tr)\n",
    "\n",
    "    for pat, supAB in patterns.items():\n",
    "        # берём только те паттерны, где есть наша метка\n",
    "        if label_id not in pat:\n",
    "            continue\n",
    "        # всего элементов в паттерне = |A|+1(label)\n",
    "        if len(pat) < 3 or len(pat) > 5:\n",
    "            continue\n",
    "\n",
    "        feat = pat - {label_id}   # вот наш A (только фичи)\n",
    "        # в A минимум 2 элемента\n",
    "        if len(feat) < 2:\n",
    "            continue\n",
    "\n",
    "        # **новая строка**: пересчёт supA по списку транзакций\n",
    "        supA = sum(1 for tr in transactions if feat.issubset(tr))\n",
    "        if supA < MIN_SUPPORT_ATTACK:\n",
    "            continue\n",
    "\n",
    "        conf = supAB / supA\n",
    "        if conf < min_conf:\n",
    "            continue\n",
    "\n",
    "        lift = conf / (sup_label/total_tx) if sup_label>0 else np.nan\n",
    "        if abs(lift - 1) <= 0.15:\n",
    "            continue\n",
    "\n",
    "        rules.append((feat, supAB, conf, lift))\n",
    "\n",
    "    print(f\"[Rules] Generated {len(rules)} filtered rules for attacks\")\n",
    "    return rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce4cbbef-e43f-48e6-a7b4-54a59715dad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CSV] Initialized CSV files.\n",
      "[Window] Total windows: 8\n",
      "\n",
      "[Window 1/8] 2017-07-07 03:30:00 -> 2017-07-07 03:50:00\n",
      "[Window 1] Transactions: 1972\n",
      "[Window 1] After filter: 1972 non-empty\n",
      "[Window 1] Starting MDFP mining…\n",
      "[Window 1] MDFP done in 1.1s, patterns: 3254\n",
      "[Filter] Closed patterns count: 227\n",
      "[Rules] Generated 0 filtered rules for attacks\n",
      "[Window 1] CSV update done.\n",
      "\n",
      "[Window 2/8] 2017-07-07 03:40:00 -> 2017-07-07 04:00:00\n",
      "[Window 2] Transactions: 4680\n",
      "[Window 2] After filter: 4680 non-empty\n",
      "[Window 2] Starting MDFP mining…\n",
      "[Window 2] MDFP done in 3.9s, patterns: 8714\n",
      "[Filter] Closed patterns count: 582\n",
      "[Rules] Generated 618 filtered rules for attacks\n",
      "[Window 2] CSV update done.\n",
      "\n",
      "[Window 3/8] 2017-07-07 03:50:00 -> 2017-07-07 04:10:00\n",
      "[Window 3] Transactions: 13326\n",
      "[Window 3] After filter: 13326 non-empty\n",
      "[Window 3] Starting MDFP mining…\n",
      "[Window 3] MDFP done in 12.6s, patterns: 11526\n",
      "[Filter] Closed patterns count: 772\n",
      "[Rules] Generated 668 filtered rules for attacks\n",
      "[Window 3] CSV update done.\n",
      "\n",
      "[Window 4/8] 2017-07-07 04:00:00 -> 2017-07-07 04:20:00\n",
      "[Window 4] Transactions: 15808\n",
      "[Window 4] After filter: 15808 non-empty\n",
      "[Window 4] Starting MDFP mining…\n",
      "[Window 4] MDFP done in 15.1s, patterns: 12659\n",
      "[Filter] Closed patterns count: 1198\n",
      "[Rules] Generated 803 filtered rules for attacks\n",
      "[Window 4] CSV update done.\n",
      "\n",
      "[Window 5/8] 2017-07-07 04:10:00 -> 2017-07-07 04:30:00\n",
      "[Window 5] Transactions: 7442\n",
      "[Window 5] After filter: 7442 non-empty\n",
      "[Window 5] Starting MDFP mining…\n",
      "[Window 5] MDFP done in 6.5s, patterns: 10787\n",
      "[Filter] Closed patterns count: 710\n",
      "[Rules] Generated 773 filtered rules for attacks\n",
      "[Window 5] CSV update done.\n",
      "\n",
      "[Window 6/8] 2017-07-07 04:20:00 -> 2017-07-07 04:40:00\n",
      "[Window 6] Transactions: 1440\n",
      "[Window 6] After filter: 1440 non-empty\n",
      "[Window 6] Starting MDFP mining…\n",
      "[Window 6] MDFP done in 0.7s, patterns: 3065\n",
      "[Filter] Closed patterns count: 301\n",
      "[Rules] Generated 0 filtered rules for attacks\n",
      "[Window 6] CSV update done.\n",
      "\n",
      "[Window 7/8] 2017-07-07 04:30:00 -> 2017-07-07 04:50:00\n",
      "[Window 7] Transactions: 1226\n",
      "[Window 7] After filter: 1226 non-empty\n",
      "[Window 7] Starting MDFP mining…\n",
      "[Window 7] MDFP done in 0.6s, patterns: 2527\n",
      "[Filter] Closed patterns count: 229\n",
      "[Rules] Generated 0 filtered rules for attacks\n",
      "[Window 7] CSV update done.\n",
      "\n",
      "[Window 8/8] 2017-07-07 04:40:00 -> 2017-07-07 05:00:00\n",
      "[Window 8] Transactions: 1588\n",
      "[Window 8] After filter: 1588 non-empty\n",
      "[Window 8] Starting MDFP mining…\n",
      "[Window 8] MDFP done in 0.8s, patterns: 3036\n",
      "[Filter] Closed patterns count: 256\n",
      "[Rules] Generated 0 filtered rules for attacks\n",
      "[Window 8] CSV update done.\n",
      "\n",
      "[Process] All windows processed. CSV files are ready.\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Sliding Window Loop and CSV Export\n",
    "\n",
    "import time\n",
    "import csv\n",
    "\n",
    "# helper: превращаем \"Col:val\" → \"Col=val\" (без _bin)\n",
    "def format_item(item_str):\n",
    "    col, val = item_str.split(':', 1)\n",
    "    return f\"{col}={val}\"\n",
    "\n",
    "# Инициализация CSV-файлов\n",
    "with open('frequent_itemsets.csv', 'w', newline='') as f:\n",
    "    csv.writer(f).writerow(['Start','End','Pattern','Support'])\n",
    "with open('closed_patterns.csv', 'w', newline='') as f:\n",
    "    csv.writer(f).writerow(['Start','End','ClosedPattern','Support'])\n",
    "with open('association_rules.csv', 'w', newline='') as f:\n",
    "    csv.writer(f).writerow(['Start','End','Antecedent','Consequent','Support','Confidence','Lift'])\n",
    "print(\"[CSV] Initialized CSV files.\")\n",
    "\n",
    "# Генерируем окна\n",
    "windows = []\n",
    "cur = df['Timestamp'].min()\n",
    "end = df['Timestamp'].max()\n",
    "while cur + window_size <= end:\n",
    "    windows.append((cur, cur + window_size))\n",
    "    cur += slide_size\n",
    "print(f\"[Window] Total windows: {len(windows)}\")\n",
    "\n",
    "# Обработка по окнам\n",
    "for idx, (t0, t1) in enumerate(windows, 1):\n",
    "    print(f\"\\n[Window {idx}/{len(windows)}] {t0} -> {t1}\")\n",
    "    df_w = df[(df['Timestamp'] >= t0) & (df['Timestamp'] < t1)]\n",
    "    trans = make_transactions_ids(df_w)\n",
    "    print(f\"[Window {idx}] Transactions: {len(trans)}\")\n",
    "    filt = [[i for i in tr if i in valid_global] for tr in trans]\n",
    "    non_empty = sum(1 for tr in filt if tr)\n",
    "    print(f\"[Window {idx}] After filter: {non_empty} non-empty\")\n",
    "\n",
    "    # майнинг MDFP с таймером\n",
    "    print(f\"[Window {idx}] Starting MDFP mining…\")\n",
    "    t_start = time.time()\n",
    "    mdfp_pats = mine_mdfp(filt, MIN_SUPPORT)\n",
    "    t_end = time.time()\n",
    "    print(f\"[Window {idx}] MDFP done in {t_end-t_start:.1f}s, patterns: {len(mdfp_pats)}\")\n",
    "\n",
    "    MIN_SUPPORT_ATTACK = 5\n",
    "    MIN_CONF_ATTACK    = 0.6\n",
    "\n",
    "    closed = filter_closed(mdfp_pats)\n",
    "# генерируем правила\n",
    "    rules = generate_rules(\n",
    "        mdfp_pats,\n",
    "        MIN_CONF_ATTACK,\n",
    "        filt,                            # список транзакций в этом окне\n",
    "        item2id['Label:DDoS']            # ID нашей метки атаки\n",
    ")\n",
    "\n",
    "\n",
    "    # Export frequent patterns\n",
    "    with open('frequent_itemsets.csv', 'a', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        for p, sup in mdfp_pats.items():\n",
    "            pattern_str = '[' + ', '.join(format_item(id2item[i]) for i in p) + ']'\n",
    "            writer.writerow([t0, t1, pattern_str, sup])\n",
    "\n",
    "    # Export closed patterns (sd) with support\n",
    "    with open('closed_patterns.csv', 'a', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        for p, sup in closed.items():\n",
    "            pattern_str = '{' + ', '.join(format_item(id2item[i]) for i in p) + '}'\n",
    "            writer.writerow([t0, t1, pattern_str, sup])\n",
    "\n",
    "    # Export association rules with full columns\n",
    "    with open('association_rules.csv', 'a', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        for A, supAB, conf, lift in rules:\n",
    "            ants = '{' + ', '.join(format_item(id2item[i]) for i in A) + '}'\n",
    "            writer.writerow([t0, t1, ants, 'DDOS', supAB, f\"{conf:.2f}\", f\"{lift:.2f}\"])\n",
    "\n",
    "\n",
    "    print(f\"[Window {idx}] CSV update done.\")\n",
    "\n",
    "print(\"\\n[Process] All windows processed. CSV files are ready.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
