{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b9a31e6-bea6-40cb-808b-06958d441bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1. Импорты и константы\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "\n",
    "# параметры дискретизации\n",
    "BINNING_SPEC = {\n",
    "    'Total Fwd Packets':      ('uniform', 5),\n",
    "    'Total Backward Packets': ('uniform', 5),\n",
    "    'Total Length of Fwd Packets': ('uniform', 5),\n",
    "    'Total Length of Bwd Packets': ('uniform', 5),\n",
    "    'Active Mean':            ('uniform', 5),\n",
    "    'Idle Mean':              ('uniform', 5),\n",
    "    'Flow Duration':          ('uniform', 5)\n",
    "}\n",
    "\n",
    "SELECTED_COLS = [\n",
    "    'Total Fwd Packets','Total Backward Packets',\n",
    "    'Total Length of Fwd Packets','Total Length of Bwd Packets',\n",
    "    'Active Mean','Idle Mean','Flow Duration',\n",
    "    'Destination IP','Destination Port',\n",
    "    'FIN Flag Count','SYN Flag Count',\n",
    "    'RST Flag Count','PSH Flag Count','ACK Flag Count'\n",
    "]\n",
    "\n",
    "FEATURE_GROUPS = {\n",
    "    'network': [\n",
    "        'Destination IP','Destination Port',\n",
    "        'FIN Flag Count','SYN Flag Count',\n",
    "        'RST Flag Count','PSH Flag Count','ACK Flag Count'\n",
    "    ],\n",
    "    'temporal': ['Flow Duration'],\n",
    "    'traffic': [\n",
    "        'Total Fwd Packets','Total Backward Packets',\n",
    "        'Total Length of Fwd Packets','Total Length of Bwd Packets',\n",
    "        'Active Mean','Idle Mean'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Порог срабатывания: сколько отфильтрованных правил нужно «пробить»\n",
    "MIN_RULES = 150\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbcf168d-03a7-43d5-b70c-0b3cd66da138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2. Функции дискретизации и извлечения фич для одной строки\n",
    "\n",
    "def discretize_columns(df_in, spec, cols):\n",
    "    df2 = df_in[['Timestamp'] + cols].copy()\n",
    "    for col, (method, bins) in spec.items():\n",
    "        if col not in cols: \n",
    "            continue\n",
    "        arr = df2[col].astype(float)\n",
    "        labels = [f\"bin{i}\" for i in range(bins)]\n",
    "        if method == 'quantile':\n",
    "            df2[col] = pd.qcut(arr, q=bins, labels=labels, duplicates='drop').astype(str)\n",
    "        else:\n",
    "            df2[col] = pd.cut(arr, bins=bins, labels=labels).astype(str)\n",
    "    return df2\n",
    "\n",
    "def row_features(raw, disc):\n",
    "    \"\"\"\n",
    "    Собирает все SELECTED_COLS в формат 'Feature=val' для одной строки.\n",
    "    \"\"\"\n",
    "    feats = set()\n",
    "    for col in SELECTED_COLS:\n",
    "        if col in BINNING_SPEC:\n",
    "            feats.add(f\"{col}={disc[col]}\")\n",
    "        else:\n",
    "            feats.add(f\"{col}={raw[col]}\")\n",
    "    return feats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e012bfb3-82fd-4330-aaeb-18fedbd24598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3. Загрузка и дискретизация датасетов\n",
    "\n",
    "# 3.1 размеченный (для оценки)\n",
    "df_labeled = pd.read_csv(\n",
    "    r'C:\\Users\\Гребенников Матвей\\Desktop\\Диплом\\Диплом\\Code\\diplom-project\\diplom\\test\\result\\Date\\Friday-WorkingHours-Afternoon-DDos.pcap_ISCX_sampled_v3.csv',\n",
    "    parse_dates=['Timestamp']\n",
    ")\n",
    "\n",
    "# 3.2 неразмеченный (для реального теста, но здесь не нужен)\n",
    "df_unlabeled = pd.read_csv(\n",
    "    r'C:\\Users\\Гребенников Матвей\\Desktop\\Диплом\\Диплом\\Code\\diplom-project\\diplom\\test\\result\\Date\\Friday-WorkingHours-Afternoon-DDos.pcap_ISCX_sampled_v4.csv',\n",
    "    parse_dates=['Timestamp']\n",
    ")\n",
    "\n",
    "# 3.3 дискретизируем оба\n",
    "df_disc_labeled   = discretize_columns(df_labeled,   BINNING_SPEC, SELECTED_COLS)\n",
    "df_disc_unlabeled = discretize_columns(df_unlabeled, BINNING_SPEC, SELECTED_COLS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be0bd5ca-d35f-495d-8710-794301e69dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4. Загрузка Top-K-шаблона и сбор DDoS-фич\n",
    "\n",
    "tmpl_df = pd.read_csv('generated_attack_templates.csv')  # поля: Attack, Group, Values\n",
    "ddos_template = set()\n",
    "\n",
    "for _, row in tmpl_df[tmpl_df['Attack']=='DDOS'].iterrows():\n",
    "    # 'Values' может быть \"Flow Duration=bin0|bin3\" или \"Destination IP=192.168.10.50\"\n",
    "    col, bins = row['Values'].split('=',1)\n",
    "    for b in bins.split('|'):\n",
    "        ddos_template.add(f\"{col}={b}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25ecd1e1-eaf0-4832-aae4-4a7500a51b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего raw_rules: 1293, после filter len(A)>=2 и по шаблону: 419\n"
     ]
    }
   ],
   "source": [
    "# ─── Cell 5. Загрузка неразмеченных правил и фильтрация по шаблону ─────────────\n",
    "\n",
    "\n",
    "\n",
    "# читаем AR-правила без меток\n",
    "rules_df = pd.read_csv('association_rules_no_label.csv')  # Antecedent, Consequent, Support, Confidence, Lift\n",
    "raw_rules = []\n",
    "for _, r in rules_df.iterrows():\n",
    "    A = set(x.strip() for x in r['Antecedent'].strip('{}').split(','))\n",
    "    B = r['Consequent']\n",
    "    sup  = r['Support']\n",
    "    conf = r['Confidence']\n",
    "    lift = r['Lift']\n",
    "    raw_rules.append((A, B, sup, conf, lift))\n",
    "\n",
    "# теперь фильтруем по шаблону и по длине A ≥ 2\n",
    "filtered_rules = [\n",
    "    (A, B)\n",
    "    for (A, B, sup, conf, lift) in raw_rules\n",
    "    # правило должно состоять минимум из 2 признаков\n",
    "    if len(A) >= 4\n",
    "    # все его фичи лежат в DDoS-шаблоне\n",
    "    and A.issubset(ddos_template)\n",
    "    # и сам consequent должен быть в шаблоне\n",
    "    and (B in ddos_template)\n",
    "]\n",
    "\n",
    "print(f\"Всего raw_rules: {len(raw_rules)}, после filter len(A)>=2 и по шаблону: {len(filtered_rules)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adecc984-1c44-4406-ada1-4de762b41769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row‐level Precision: 0.89, Recall: 0.83\n",
      "TP=24907, FP=3084, TN=66916, FN=5093\n",
      "Готово: row_level_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "# Cell 6. Детекция по строкам и оценка качества\n",
    "\n",
    "preds = []\n",
    "TP = FP = TN = FN = 0\n",
    "\n",
    "for idx, raw in df_labeled.iterrows():\n",
    "    disc  = df_disc_labeled.loc[idx]\n",
    "    feats = row_features(raw, disc)\n",
    "\n",
    "    # обязательная проверка: хотя бы по одному фичу из каждой группы\n",
    "    ok = True\n",
    "    for grp, cols in FEATURE_GROUPS.items():\n",
    "        # берем из шаблона только фичи этой группы\n",
    "        grp_feats = {f for f in ddos_template if any(f.startswith(c+\"=\") for c in cols)}\n",
    "        if not (grp_feats & feats):\n",
    "            ok = False\n",
    "            break\n",
    "\n",
    "    if not ok:\n",
    "        pred = 'BENIGN'\n",
    "    else:\n",
    "        # считаем, сколько отфильтрованных правил ≡ A⇒B 'прошли'\n",
    "        fires = sum(1 for A,B in filtered_rules if A.issubset(feats) and (B in feats))\n",
    "        pred  = 'DDoS' if fires >= MIN_RULES else 'BENIGN'\n",
    "\n",
    "    true = raw['Label']\n",
    "    if pred == true:\n",
    "        if pred == 'DDoS': TP += 1\n",
    "        else:             TN += 1\n",
    "    else:\n",
    "        if pred == 'DDoS': FP += 1\n",
    "        else:             FN += 1\n",
    "\n",
    "    rec = raw.to_dict()\n",
    "    rec['Predicted'] = pred\n",
    "    preds.append(rec)\n",
    "\n",
    "precision = TP / (TP + FP) if TP + FP else 0\n",
    "recall    = TP / (TP + FN) if TP + FN else 0\n",
    "print(f\"Row‐level Precision: {precision:.2f}, Recall: {recall:.2f}\")\n",
    "print(f\"TP={TP}, FP={FP}, TN={TN}, FN={FN}\")\n",
    "\n",
    "# сохраняем все строки с предсказаниями\n",
    "pd.DataFrame(preds).to_csv('row_level_predictions.csv', index=False)\n",
    "print(\"Готово: row_level_predictions.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09f391f8-b9fe-40a3-abfa-944be6f0254d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    min_len  min_rules  num_rules     TP     FP     TN     FN  precision  \\\n",
      "0         4        150        419  24907   3084  66916   5093   0.889822   \n",
      "1         3        200        682  24907  15227  54773   5093   0.620596   \n",
      "2         4        100        419  24907  15227  54773   5093   0.620596   \n",
      "3         2        200        771  24907  15467  54533   5093   0.616907   \n",
      "4         3        150        682  24907  16074  53926   5093   0.607769   \n",
      "5         2        150        771  24907  17107  52893   5093   0.592826   \n",
      "6         4        200        419  13636    427  69573  16364   0.969637   \n",
      "7         4         50        419  29558  39072  30928    442   0.430686   \n",
      "8         3         50        682  29952  40545  29455     48   0.424869   \n",
      "9         2         50        771  29952  40555  29445     48   0.424809   \n",
      "10        3        250        682  13636   3854  66146  16364   0.779646   \n",
      "11        2        100        771  25878  39077  30923   4122   0.398399   \n",
      "12        3        100        682  25663  39064  30936   4337   0.396481   \n",
      "13        2        250        771  13636  13066  56934  16364   0.510673   \n",
      "14        4        250        419     43      0  70000  29957   1.000000   \n",
      "15        5         50          0      0      0  70000  30000   0.000000   \n",
      "16        5        100          0      0      0  70000  30000   0.000000   \n",
      "17        5        150          0      0      0  70000  30000   0.000000   \n",
      "18        5        200          0      0      0  70000  30000   0.000000   \n",
      "19        5        250          0      0      0  70000  30000   0.000000   \n",
      "\n",
      "      recall        f1  \n",
      "0   0.830233  0.858995  \n",
      "1   0.830233  0.710269  \n",
      "2   0.830233  0.710269  \n",
      "3   0.830233  0.707847  \n",
      "4   0.830233  0.701793  \n",
      "5   0.830233  0.691727  \n",
      "6   0.454533  0.618932  \n",
      "7   0.985267  0.599371  \n",
      "8   0.998400  0.596077  \n",
      "9   0.998400  0.596018  \n",
      "10  0.454533  0.574268  \n",
      "11  0.862600  0.545058  \n",
      "12  0.855433  0.541831  \n",
      "13  0.454533  0.480971  \n",
      "14  0.001433  0.002863  \n",
      "15  0.000000  0.000000  \n",
      "16  0.000000  0.000000  \n",
      "17  0.000000  0.000000  \n",
      "18  0.000000  0.000000  \n",
      "19  0.000000  0.000000  \n",
      "Saved → gridsearch_len_rules.csv\n"
     ]
    }
   ],
   "source": [
    "# ─── Cell X+: Grid Search по min_len и min_rules ─────────────────────────────\n",
    "import pandas as pd\n",
    "from itertools import product\n",
    "\n",
    "# Диапазоны, которые хотим перебрать\n",
    "min_len_list   = [2, 3, 4, 5]      # минимальная длина Antecedent\n",
    "min_rules_list = [50, 100, 150, 200, 250]  # пороги для fires >= min_rules\n",
    "\n",
    "results = []\n",
    "\n",
    "for min_len, min_rules in product(min_len_list, min_rules_list):\n",
    "    # 1) Отбираем правила по длине A >= min_len\n",
    "    filtered = [\n",
    "        (A, B)\n",
    "        for (A, B, sup, conf, lift) in raw_rules\n",
    "        if len(A) >= min_len\n",
    "           and A.issubset(ddos_template)\n",
    "           and (B in ddos_template)\n",
    "    ]\n",
    "\n",
    "    # 2) Считаем метрики на уровне строк\n",
    "    TP = FP = TN = FN = 0\n",
    "    for idx, raw in df_labeled.iterrows():\n",
    "        disc  = df_disc_labeled.loc[idx]\n",
    "        feats = row_features(raw, disc)\n",
    "\n",
    "        # групповой фильтр: хотя бы одна фича из каждой группы\n",
    "        ok = all(\n",
    "            (set(f for f in ddos_template if any(f.startswith(c+\"=\") for c in cols)) & feats)\n",
    "            for grp, cols in FEATURE_GROUPS.items()\n",
    "        )\n",
    "        if not ok:\n",
    "            pred = 'BENIGN'\n",
    "        else:\n",
    "            fires = sum(1 for A, B in filtered if A.issubset(feats) and (B in feats))\n",
    "            pred  = 'DDoS' if fires >= min_rules else 'BENIGN'\n",
    "\n",
    "        true = raw['Label']\n",
    "        if pred == true:\n",
    "            if pred == 'DDoS': TP += 1\n",
    "            else:             TN += 1\n",
    "        else:\n",
    "            if pred == 'DDoS': FP += 1\n",
    "            else:             FN += 1\n",
    "\n",
    "    precision = TP / (TP + FP) if TP + FP else 0\n",
    "    recall    = TP / (TP + FN) if TP + FN else 0\n",
    "    f1        = 2 * precision * recall / (precision + recall) if (precision + recall) else 0\n",
    "\n",
    "    results.append({\n",
    "        'min_len': min_len,\n",
    "        'min_rules': min_rules,\n",
    "        'num_rules': len(filtered),\n",
    "        'TP': TP, 'FP': FP, 'TN': TN, 'FN': FN,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    })\n",
    "\n",
    "# 3) Собираем и сортируем\n",
    "df_res = pd.DataFrame(results)\n",
    "df_res = df_res.sort_values(['f1','precision'], ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(df_res)\n",
    "\n",
    "# Сохраняем в CSV для дальнейшего анализа\n",
    "df_res.to_csv('gridsearch_len_rules.csv', index=False)\n",
    "print(\"Saved → gridsearch_len_rules.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1869505c-195e-4384-a3a4-fcb8826ab154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved misclassification_reasons.csv — разбор каждого FP/FN\n"
     ]
    }
   ],
   "source": [
    "# ─── Cell Z: Диагностика причин FP/FN ───────────────────────────────────────\n",
    "import pandas as pd\n",
    "\n",
    "# ← Подставь сюда те же константы, что в своём detection-скрипте\n",
    "MIN_RULES     = 150    # сколько правил должно «пробиться»\n",
    "RULE_MIN_LEN  = 4    # отбрасываем слишком короткие A (len(A) >= RULE_MIN_LEN)\n",
    "\n",
    "# 1) Подгружаем предсказания\n",
    "df_preds = pd.read_csv('row_level_predictions.csv', parse_dates=['Timestamp'])\n",
    "\n",
    "# 2) Загружаем «сырые» AR-правила (без меток), которые ты фильтруешь в коде\n",
    "rules_df = pd.read_csv('association_rules_no_label_short.csv')  # или свой файл\n",
    "raw_rules = []\n",
    "for _, r in rules_df.iterrows():\n",
    "    A = set(x.strip() for x in r['Antecedent'].strip('{}').split(','))\n",
    "    B = r['Consequent']  # здесь у тебя просто 'DDOS' или 'BENIGN'\n",
    "    raw_rules.append((A, B))\n",
    "\n",
    "# 3) Отфильтровываем их ровно так же, как ты это делаешь в detect_attack \n",
    "#    (len(A)>=RULE_MIN_LEN, A⊆ddos_template и B∈ddos_template)\n",
    "filtered_rules = [\n",
    "    (A, B)\n",
    "    for A, B in raw_rules\n",
    "    if len(A) >= RULE_MIN_LEN\n",
    "       and A.issubset(ddos_template)\n",
    "       and (B in ddos_template)\n",
    "]\n",
    "\n",
    "# 4) Раскладываем ddos_template по группам, чтобы проверять покрытие network/temporal/traffic\n",
    "ddos_template_groups = {\n",
    "    grp: {f for f in ddos_template if any(f.startswith(col + '=') for col in cols)}\n",
    "    for grp, cols in FEATURE_GROUPS.items()\n",
    "}\n",
    "\n",
    "# 5) Собираем diagnostics\n",
    "records = []\n",
    "for idx, row in df_preds.iterrows():\n",
    "    true = row['Label']\n",
    "    pred = row['Predicted']\n",
    "    # только ошибки\n",
    "    if true == pred:\n",
    "        continue\n",
    "\n",
    "    # 5.1) Фичи из исходных и дискретных\n",
    "    raw  = df_labeled.loc[idx]\n",
    "    disc = df_disc_labeled.loc[idx]\n",
    "    feats = row_features(raw, disc)\n",
    "\n",
    "    # 5.2) Какие группы шаблона вообще не покрылись?\n",
    "    missing = [\n",
    "        grp for grp, tmpl_feats in ddos_template_groups.items()\n",
    "        if not (tmpl_feats & feats)\n",
    "    ]\n",
    "\n",
    "    # 5.3) Какие filtered_rules пробились?\n",
    "    fired = [A for A, B in filtered_rules if A.issubset(feats)]\n",
    "    n_fired = len(fired)\n",
    "\n",
    "    # 5.4) Формируем причину\n",
    "    if missing:\n",
    "        reason = f\"Missing template groups: {missing}\"\n",
    "    elif n_fired < MIN_RULES:\n",
    "        reason = f\"Only {n_fired} rules fired (<{MIN_RULES})\"\n",
    "    else:\n",
    "        reason = \"Passed all checks but still misclassified\"\n",
    "\n",
    "    records.append({\n",
    "        'row_index':        idx,\n",
    "        'true_label':       true,\n",
    "        'predicted':        pred,\n",
    "        'missing_groups':   missing,\n",
    "        'num_rules_fired':  n_fired,\n",
    "        'fired_antecedents': fired,\n",
    "        'reason':           reason\n",
    "    })\n",
    "\n",
    "# 6) Сохраняем\n",
    "df_diag = pd.DataFrame(records)\n",
    "df_diag.to_csv('misclassification_reasons.csv', index=False)\n",
    "print(\"✓ Saved misclassification_reasons.csv — разбор каждого FP/FN\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
